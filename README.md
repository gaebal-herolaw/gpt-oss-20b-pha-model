# gpt-oss-20b-PHA Model

Personal Health Agent Q&A System using RAG (Retrieval Augmented Generation)

## ğŸ¯ Project Overview

This project implements a RAG-based question-answering system for Personal Health Agent research papers using the gpt-oss-20b model.

### Key Features

- âœ… Local GPU-accelerated inference
- âœ… Multi-lingual embedding support (Korean + English)
- âœ… ChromaDB vector store for efficient retrieval
- âœ… FastAPI REST API server
- âœ… Interactive CLI interface
- âœ… Source tracking and citation

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- NVIDIA GPU with CUDA support (recommended: RTX Pro 6000 96GB)
- CUDA 11.8+ or 12.1+
- 32GB+ RAM
- 100GB+ free disk space

### Installation

```bash
# Clone the repository
git clone https://github.com/gaebal-herolaw/gpt-oss-20b-pha-model.git
cd gpt-oss-20b-pha-model

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: .\venv\Scripts\Activate.ps1

# Install PyTorch with CUDA support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install other dependencies
pip install -r requirements.txt

# Create .env file
cp .env.example .env
```

### Build Vector Database

```bash
python build_index.py
```

### Run CLI

```bash
python main.py
```

### Run API Server

```bash
python api_server.py
```

API will be available at `http://localhost:8000`

Swagger UI: `http://localhost:8000/docs`

## ğŸ“ Project Structure

```
gpt-oss-20b-pha-model/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Configuration
â”‚   â”œâ”€â”€ embedding_model.py     # Embedding model
â”‚   â”œâ”€â”€ local_llm.py          # Local LLM
â”‚   â”œâ”€â”€ data_processor.py     # Data processing
â”‚   â”œâ”€â”€ vector_store.py       # Vector store
â”‚   â””â”€â”€ rag_chain.py          # RAG pipeline
â”œâ”€â”€ data/                      # Research papers (add your files here)
â”œâ”€â”€ main.py                    # CLI interface
â”œâ”€â”€ build_index.py            # Vector DB builder
â”œâ”€â”€ api_server.py             # FastAPI server
â”œâ”€â”€ requirements.txt          # Dependencies
â””â”€â”€ README.md                 # This file
```

## ğŸ”§ Configuration

Edit `src/config.py` to customize:

- Model paths and settings
- Batch sizes for GPU optimization
- Vector database parameters
- API server settings

## ğŸ“š Usage Examples

### CLI Mode

```python
ì§ˆë¬¸: Personal Health Agentì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?

ë‹µë³€:
Personal Health AgentëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
...
```

### API Mode

```bash
curl -X POST "http://localhost:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{"query": "What are the key features of Personal Health Agent?", "k": 5}'
```

## ğŸ› ï¸ Troubleshooting

### GPU Not Detected

```bash
# Check CUDA
nvidia-smi

# Verify PyTorch CUDA
python -c "import torch; print(torch.cuda.is_available())"
```

### Memory Issues

Reduce batch sizes in `src/config.py`:

```python
EMBEDDING_BATCH_SIZE = 32  # Reduce from 64
GENERATION_BATCH_SIZE = 4  # Reduce from 8
```

## ğŸ“Š Performance Benchmarks

| Task | Time | VRAM Usage |
|------|------|-----------|
| Embedding Model Load | 5-10s | ~2GB |
| LLM Load (20B) | 30-60s | ~40GB |
| Vector DB Build | 5-10min | ~5GB |
| Query Search | <1s | ~0.1GB |
| Answer Generation | 5-15s | ~42GB |

## ğŸ“„ License

This project is for research and educational purposes.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“® Contact

For questions or issues, please open a GitHub issue.
